<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Things I’m Learning While Training SuperHOT | kaiokendev.github.io</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Things I’m Learning While Training SuperHOT" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/til" />
<meta property="og:url" content="http://localhost:4000/til" />
<meta property="og:site_name" content="kaiokendev.github.io" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Things I’m Learning While Training SuperHOT" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Things I’m Learning While Training SuperHOT","url":"http://localhost:4000/til"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=HEAD">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://localhost:4000/">kaiokendev.github.io</a></h1>
      

      <h1 id="things-im-learning-while-training-superhot">Things I’m Learning While Training SuperHOT</h1>

<p>I have been working on SuperHOT for some time now. It is a fiction-focused finetune of LLaMa with extra focus towards NSFW outputs while also being capable of general use instructions. The main reason I’m making the model is because it is fun and serves as a good way to learn the inner workings of Transformers, dataset creation and techniques, and probing the capabilities of LLMs. There are also a lot of people who want NSFW-capable models, and they provide useful, honest feedback, especially when they don’t get what they want. Besides, it’s a fun model to use.</p>

<p>I’m making this page to share some of my findings in the hope that others might find it useful. I will update this page as time goes on with any information that might be important and whenever I have the time.</p>

<h2 id="background">Background</h2>
<p>Originally, I was working on a <a href="https://github.com/hwchase17/langchain">Langchain</a> extension for oobabooga’s <a href="https://github.com/oobabooga/text-generation-webui">Text-Generation-WebUI</a>. It was not a very fun project. Gradio is not fun to work with when it comes to stateful UI updates, or even encapsulation of UI and logic. I ended up hand-rolling my own UI state management system just to give a nice user experience when modifying templates, chains, etc. After some time of using Langchain, I realized I was only using a subset of the features (because they were the most useful to me), and even those features could be replicated outside of the framework very easily and save me from the unnecessary bloat. I was also displeased with the quality of the chained outputs, so I looked for alternate ways to improve the generation quality. I ended up making <a href="https://huggingface.co/kaiokendev/SuperCOT-LoRA">SuperCOT</a> at this time, by combining parts of datasets from Sahil Chaudhary’s <a href="https://github.com/sahil280114/codealpaca">Code Alpaca</a>, CMU Mellon’s NeuLab <a href="https://conala-corpus.github.io/">CoNaLa</a>, Google’s <a href="https://github.com/google-research/FLAN">FLAN (QED and Aqua)</a>, and Peng et. al.’s <a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Alpaca GPT-4</a>, mostly sourced from Qingsi Yi et. al’s <a href="https://github.com/PhoebusSi/Alpaca-CoT">Alpaca CoT</a> project dataset</p>

<p>The resulting model worked much better with chained prompting for me, but the quality was still a far cry from the demonstrations in Langchain’s <a href="https://python.langchain.com/en/latest/modules/agents/getting_started.html">documentation</a>. I stopped working on the extension, instead I started playing with parts of the framework in isolation, such as <a href="https://github.com/kaiokendev/superbig">vector databases</a> and making my own lightweight chained prompting wrapper library. In the meantime, apparently some users found the model was very good at producing NSFW content. I had made it a point to filter the refusals and bias from the dataset as best as I could, so it was not unexpected. Soon after, the idea was floated to make models based on solely online roleplay logs, with the idea that such models would be much better at making chatbot outputs and would require no filtering of refusals. So, I started working on SuperHOT, and in the meantime others also worked on their own models, such as <a href="https://huggingface.co/reeducator/bluemoonrp-13b">Bluemoon 13</a>/<a href="https://huggingface.co/reeducator/bluemoonrp-30b">30B</a>.</p>

<h2 id="lima-works">LIMA Works</h2>

<p>Inspired by <a href="https://arxiv.org/pdf/2305.11206.pdf">LIMA</a>, SuperHOT contains only 1200 examples (so far in the current prototype version I am using). Despite such a low sample size, the model performs very well in my testing:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/superhot13b.PNG" alt="SuperHOT Early Prototype Multi Instruct" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Prototype of SuperHOT with a multi-instruct prompt</em></td>
    </tr>
  </tbody>
</table>

<p>It is able to generalize well to unseen tasks, and in cases where it does not, I was able to add just a dozen or so samples to train the model to handle the intended behavior (which it generalized as well). The results to me show that making minimalist datasets is more than enough, especially when you have a specific list of functionalities in mind. We can see from LIMA that adding more data does not necessarily correlate with better output quality:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="limaquality.PNG" alt="LIMA Quality 7B" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>LIMA applied to 7B LLaMa model shows that output quality does not appear to scale with sample quantity when measured by ChatGPT</em></td>
    </tr>
  </tbody>
</table>

<p>This is not entirely good news though – one of the main points made in the LIMA paper is that the model’s core knowledge comes mainly from its pre-training corpus, and that finetuning the model merely affects the style. Put another way, you might need several billions of related tokens in order to impart new useful knowledge into the model. I would love to see a paper measuring how few tokens are needed to teach a pre-trained model new factual information. Another tidbit is to take LIMA’s GPT preference weighting with a grain of salt, as outlined recently in <a href="https://arxiv.org/abs/2305.17926">Wang et al.</a> To address two points made in the LIMA paper:</p>

<blockquote>
  <p>LIMA is not as robust as product-grade models; while LIMA typically generates good responses, an unlucky sample during decoding or an adversarial prompt can often lead to a weak response.</p>
</blockquote>

<p>I agree, although I experience this with other models that are trained on a far larger corpus as well. I observed that it’s still possible to teach the model to use chain-of-thought when constructing answers with only 100 samples. In some cases, it (SuperHOT 13B) produced the correct answer where SuperCOT 13B (which used nearly 60K samples) could not. I think the path of LIMA is a better approach for enthusiast models, especially when touching on the other point…</p>

<h2 id="exponential-quality">Exponential Quality</h2>

<blockquote>
  <p>Primarily, the mental effort in constructing such examples is significant and difficult to
scale up.</p>
</blockquote>

<p>In my case, after some revisions of the dataset, the model’s outputs eventually became so good that I was able to use it to produce even more training data.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="minecraftrecursiveinstruct.PNG" alt="Minecraft Recursive Instruct" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>SuperHOT 13B “recursive instruct” prompt generation with occasional intervention. Ignore the fact that that is definitely NOT the plot of Max Payne</em></td>
    </tr>
  </tbody>
</table>

<p>Occasionally, I would interrupt the model’s generation to seed it with a word or phrase, or lightly clean up parts of the message such as the formatting. As the model became better and better at following the prompt and producing the expected outputs, I had to intervene less and less. In fact, some of the training data right now contains 2000 token generations that I did not have to touch at all. Making those manually would have taken ages.</p>

<p>In the above snippet, I made the following changes, stopping and continuing generation when I made my updates:</p>
<ul>
  <li>“Assuming you mean Minecraft, the company responsible for”</li>
  <li>Truncating the first paragraph as it became too opinionated</li>
  <li>“As a simple example,”</li>
  <li>“Alternatively,”</li>
  <li>Truncating the list of possible player weapons</li>
</ul>

<p>I find that this form of prompt intervention combined with the fact that we’re only looking for perhaps a dozen or two dozen sets of samples to teach behaviors works very well. It’s still a monotonous task and takes a lot of time, but much less so than doing it all by hand. I even use the model to generate its own prompts, but for situations where the model is not yet ready to handle the complexity, it’s sufficient to fallback to more capable models.</p>

<h2 id="multi-instruct-is-very-good">Multi-Instruct is very good</h2>

<p>Since LIMA shows that <em>reducing</em> the training set can still yield good results, I was curious if it was possible to improve the behavior of the model by <em>enhancing</em> the depth of tasks in each sample. As in, rather than give the model 1 instruction per sample, why not provide it with 3 discrete tasks? Or 5? Or even 10+? One of the common complaints I’ve seen of local models is that they are very poor at handling situations where many instructions are provided and the model needs to provide an answer for each instruction. Maybe by showing the model examples with layered requirements, it will learn to be more mindful of little details in the prompt? With this in mind, I started adding “Multi-instruct” samples to the dataset. Some examples of multi-instruct prompts include:</p>

<ul>
  <li>
    <blockquote>
      <p>Delve into the growth and significance of the video game industry in Japan, with a particular emphasis on the following topics: the emergence of early Japanese video game companies (e.g., Nintendo, Sega), groundbreaking game consoles and their impact on the market, influential game developers and their contributions to gaming culture (e.g., Shigeru Miyamoto, Hideo Kojima), landmark titles that defined the medium (e.g., The Legend of Zelda, Final Fantasy, Street Fighter), the role of Japanese games in various popular genres (e.g., action-adventure, RPG, fighting), the rise of esports and Japanese game-related entertainment, and the competitive landscape of the industry. Address how Japanese games have influenced the larger gaming culture, both domestically and internationally.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Investigate the rise and fall of five significant world empires spanning different time periods and continents: the Egyptian Empire, the Persian Empire (Achaemenid), the Mongol Empire, the Mughal Empire, and the Russian Empire (Tsarist). Delve into each empire’s origins, central rulers or figures, territorial expansion, political and governance systems, military strategies, cultural and artistic achievements, religious and philosophical developments, and the reasons for their decline. Discuss how these empires left lasting legacies and set the stage for the emergence of modern states and the ongoing impact on international relations, cultural identities, and historical narratives.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Expound upon the history, culture, contributions, and challenges faced by five critically acclaimed philosophers from different eras and regions: ancient Greek philosophy (Socrates, Plato, or Aristotle), Eastern philosophy (Confucius, Laozi, or Zhuangzi), medieval European philosophy (St. Augustine, St. Thomas Aquinas, or William of Ockham), modern Western philosophy (René Descartes, Immanuel Kant, or David Hume), and contemporary philosophy (Ludwig Wittgenstein, Noam Chomsky, or Simone de Beauvoir). Provide a concise overview of their most notable works, main areas of inquiry, major theoretical contributions, lasting influences on later philosophers or movements, the broader cultural and historical context they were operating in, and contemporary applications or interpretations of their ideas.</p>
    </blockquote>
  </li>
</ul>

<p>With only 15 or so multi-instruct examples, the model quality became significantly better, especially when performing such layered tasks.</p>

<p>Example prompt:</p>

<blockquote>

  <p>Deconstruct five major technological innovations in 20th and 21st centuries—Internet, personal computing, mobile phones, artificial intelligence, and electric vehicles. For each innovation, provide information on their early origins, key inventors or pioneers, important milestones and breakthroughs, adoption and proliferation into mainstream use, economic effects, societal impact, ethical considerations, and potential future developments. Discuss how these innovations reshaped the way people work, communicate, and live, and explore the synergies or connections among these technologies and the potential for future transformative advancements.</p>
</blockquote>

<p>Example output:</p>

<blockquote>
  <p>• Internet: The internet was developed from a series of government-funded projects that began with ARPANET (Advanced Research Projects Agency Network) in the late 1960s. These included packet switching technology to transmit data over multiple networks, protocols such as TCP/IP, and standards like HTTP and HTML to enable communication between different computers using different operating systems. Key inventions include TCP/IP, DNS, and HTTP; key milestone events include the launching of ARPANET and the World Wide Web by Tim Berners-Lee at CERN in 1989. Adoption and proliferation have been driven by commercialization of the technology through companies like Google, Facebook, Amazon, Microsoft, Apple, etc., which have built business models around online services and content delivery. Economic effects include creation of new industries, disruption of existing ones, increased productivity, and improved access to knowledge and entertainment. Societal impact includes facilitating global connectivity, democratizing access to information, changing the nature of social interactions, and creating opportunities for entrepreneurship and self-employment. Ethical concerns include privacy issues related to collection and use of user data, censorship, surveillance, propaganda, and fake news. Potential future developments include continued expansion of the network infrastructure, development of next generation wireless technologies, greater integration of IoT devices, more powerful AI applications, and further evolution toward autonomous systems.</p>
</blockquote>

<blockquote>
  <p>• Personal Computing: Early personal computers were designed by hobbyists and enthusiasts starting in the mid-1970s, including Steve Jobs and Steve Wozniak’s Apple I and II, along with the Altair 8800 and other kits sold by MITS. Innovators include Gordon Eubanks who created the first spreadsheet program VisiCalc for the Apple II, and Mitch Kapor who wrote Lotus 1-2-3 for DOS. Key milestones include IBM introducing its first PC in 1981, followed by Windows 1.0 in 1985, the release of the Macintosh in 1984, and Apple shipping the first mass market laptop computer, the PowerBook 100, in 1991. Adopters included individuals, small businesses, schools, universities, and eventually large enterprises. Economic effects included increasing worker productivity, enabling new business models, and spurring innovation in hardware, software, and peripherals. Societal impact included empowering individuals to create digital art, music, and literature, and helping people learn new skills and pursue interests they might not otherwise be able to do without access to this type of technology. Ethical concerns included privacy issues associated with centralized databases storing sensitive personal information, security vulnerabilities leading to widespread malware attacks, and the rise of cybercrime. Potential future developments include continuing improvements in processing power, storage capacity, and battery life, along with increasingly sophisticated AI capabilities and voice interfaces.</p>
</blockquote>

<blockquote>
  <p>• Mobile Phones: GSM cellular technology was standardized in Europe in the 1980s and launched commercially in Scandinavia in 1991. It became widely adopted elsewhere during the following decade, culminating in the introduction of 3G networks and smartphones in the early 2000s. Key innovators include Joel Engel, Joe Salter, and Tomio Fujisaki who worked together to develop GSM. Other significant contributions came from Ericsson, Nokia, Motorola, Qualcomm, Samsung, and others. Milestones include the first call made by Martin Cooper of Motorola in 1983, followed by the launch of the Nordic Mobile Telephone (NMT) system in Denmark in 1987. By the end of the century, there were more than one billion subscribers worldwide. Economic effects included the emergence of new telecommunications carriers, growth of mobile apps ecosystems, and the rise of “smart” cities leveraging sensors and connected devices. Societal impact included providing ubiquitous communications for billions of people across all socioeconomic strata, enhancing safety and convenience, and improving access to education and healthcare. Ethical concerns centered on privacy and security risks posed by tracking users’ locations and activities, and exposure to electromagnetic radiation. Potential future developments include 5G networks, edge computing, blockchain, and augmented reality applications.</p>
</blockquote>

<blockquote>
  <p>• Artificial Intelligence: While research in AI dates back decades, the field gained momentum in the 1980s when expert systems and neural networks were introduced. Major milestones include the publication of seminal papers by Geoff Hinton (“Neural Networks and Learning Automata”) and Yann LeCun (“Learning Theory and an Application to Pattern Recognition”), along with groundbreaking research conducted at Carnegie Mellon University and Stanford University. Significant contributions also came from Elon Musk, Andrew Ng, Yoshua Bengio, Yoav Shoham, and many others. Economic effects included the creation of entirely new industries, such as machine learning and robotics, and the automation of jobs previously performed by humans. Societal impact included improved decision making, faster diagnosis of diseases, and better predictions about climate change and natural disasters. Ethical concerns focused on ensuring safe operation of autonomous vehicles and avoiding bias against certain groups. Potential future developments include more advanced deep learning techniques, general purpose AIs capable of performing any task given sufficient training data, and human-like robots with emotional intelligence.</p>
</blockquote>

<blockquote>
  <p>• Electric Vehicles: The modern era of electric cars can be traced back to the introduction of the Tesla Roadster in 2008, followed by the Chevy Volt in 2010 and the Nissan LEAF in 2011. Important innovations include lithium ion batteries and inductive charging. Major players include Tesla, General Motors, Volkswagen, Toyota, BYD, and many others. Economic benefits include reduced emissions and lower fuel costs compared to gasoline engines. Societal impact has included increased awareness of environmental sustainability and the need for clean energy sources. Ethical concerns center on limited range due to current battery technology, high cost relative to internal combustion engine vehicles, and lack of public charging stations. Potential future developments include solid state batteries offering longer ranges and faster recharging times, swappable batteries allowing drivers to replace worn out cells, and vehicle-to-grid capability enabling cars to act as virtual power plants.</p>
</blockquote>

<blockquote>
  <p>In summary, these five technologies have transformed our lives and continue to shape our future. They represent some of the most important innovations of the past few decades and are poised to play even larger roles going forward. Their stories offer valuable lessons about what it takes to bring truly game-changing ideas to fruition.</p>
</blockquote>

<p>The above example output only required minor intervention in the first bullet point, and no intervention in the subsequent points. Condensing the sample size while expanding the task expectation has lead to good results for me.</p>

<h3 id="citations">Citations</h3>
<p>Never forget the citations</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{yin2018mining,
  author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
  title = {Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow},
  booktitle = {International Conference on Mining Software Repositories},
  series = {MSR},
  pages = {476--486},
  year = {2018},
  publisher = {ACM},
  doi = {https://doi.org/10.1145/3196398.3196408},
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations}
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{alpaca-cot,
  author = {Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, Zheng Lin },
  school = {Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China},
  title = {Alpaca-CoT: An Instruction-Tuning Platform with Unified Interface of Instruction Collection, Parameter-efficient Methods, and Large Language Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/PhoebusSi/alpaca-CoT}},
}

@article{Zhou2023LIMALI,
  title={LIMA: Less Is More for Alignment},
  author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and L. Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11206}
}

@article{Wang2023LargeLM,
  title={Large Language Models are not Fair Evaluators},
  author={Peiyi Wang and Lei Li and Liang Chen and D. Zhu and Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.17926}
}
</code></pre></div></div>

<h2 id="changelog">Changelog</h2>
<ul>
  <li>Added ‘Background’ (6/9/23)</li>
  <li>Added ‘LIMA Works’ (6/9/23)</li>
  <li>Added ‘Citations’  (6/9/23)</li>
  <li>Added ‘Multi-Instruct’  (6/9/23)</li>
  <li>Added ‘Exponential Quality’  (6/9/23)</li>
  <li>Preparing ‘Balancing a Dataset is Hard’</li>
  <li>Preparing ‘Recursive Prompt’</li>
  <li>Preparing ‘Heuristic PPO’</li>
  <li>Preparing ‘Inverse Prompt’</li>
  <li>Preparing ‘XPos + RoPE’</li>
  <li>Some cleaning</li>
</ul>


      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>
